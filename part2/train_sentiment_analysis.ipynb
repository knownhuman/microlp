{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Part 2: Training your own ML Model\n",
    "\n",
    "<a href=\"https://colab.research.google.com/github/peckjon/hosting-ml-as-microservice/blob/master/part2/train_sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download corpuses\n",
    "\n",
    "We'll continue using the `movie_reviews` corpus to train our model. The `stopwords` corpus contains a [set of standard stopwords](https://gist.github.com/sebleier/554280) we'll want to remove from the input, and `punkt` is used for toneization in the [.words()](https://www.nltk.org/api/nltk.corpus.html#corpus-reader-functions) method of the corpus reader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\pbrad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\pbrad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pbrad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define feature extractor and bag-of-words converter\n",
    "\n",
    "Given a list of (already tokenized) words, we need a function to extract just the ones we care about: those not found in the list of English stopwords or standard punctuation.\n",
    "\n",
    "We also need a way to easily turn a list of words into a [bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model), pairing each word with the count of its occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "stopwords_eng = stopwords.words('english')\n",
    "\n",
    "def extract_features(words):\n",
    "    return [w for w in words if w not in stopwords_eng and w not in punctuation]\n",
    "\n",
    "def bag_of_words(words):\n",
    "    bag = {}\n",
    "    for w in words:\n",
    "        bag[w] = bag.get(w,0)+1\n",
    "    return bag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest, clean, and convert the positive and negative reviews\n",
    "\n",
    "For both the positive (\"pos\") and negative (\"neg\") sets of reviews, extract the features and convert to bag of words. From these, we construct a list of tuples known as a \"featureset\": the first part of each tuple is the bag of words for that review, and the second is its label (\"pos\"/\"neg\").\n",
    "\n",
    "Note that `movie_reviews.words(fileid)` provides a tokenized list of words. If we wanted the un-tokenized text, we would use `movie_reviews.raw(fileid)` instead, then tokenize it using our preferred tokenizeer (e.g. [nltk.tokenize.word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.punkt.PunktLanguageVars.word_tokenize))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews\n",
    "\n",
    "reviews_pos = []\n",
    "reviews_neg = []\n",
    "for fileid in movie_reviews.fileids('pos'):\n",
    "    words = extract_features(movie_reviews.words(fileid))\n",
    "    reviews_pos.append((bag_of_words(words), 'pos'))\n",
    "for fileid in movie_reviews.fileids('neg'):\n",
    "    words = extract_features(movie_reviews.words(fileid))\n",
    "    reviews_neg.append((bag_of_words(words), 'neg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pos'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews_pos[123][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split reviews into training and test sets\n",
    "We need to break up each group of reviews into a training set (about 80%) and a test set (the remaining 20%). In case there's some meaningful order to the reviews (e.g. the first 800 are from one group of reviewers, the next 200 are from another), we shuffle the sets first to ensure we aren't introducing additional bias. Note that this means our accuracy will not be exactly the same on every run; if you wish to see consistent results on each run, you can stabilize the shuffle by calling [random.seed(n)](https://www.geeksforgeeks.org/random-seed-in-python/) first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "\n",
    "split_pct = .80\n",
    "\n",
    "def split_set(review_set):\n",
    "    split = int(len(review_set)*split_pct)\n",
    "    return (review_set[:split], review_set[split:])\n",
    "\n",
    "shuffle(reviews_pos)\n",
    "shuffle(reviews_neg)\n",
    "\n",
    "pos_train, pos_test = split_set(reviews_pos)\n",
    "neg_train, neg_test = split_set(reviews_neg)\n",
    "\n",
    "train_set = pos_train+neg_train\n",
    "test_set = pos_test+neg_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'andy': 6,\n",
       "  'leaves': 1,\n",
       "  'cowboy': 2,\n",
       "  'camp': 2,\n",
       "  'mother': 1,\n",
       "  'holds': 1,\n",
       "  'yard': 3,\n",
       "  'sale': 2,\n",
       "  'scrounges': 1,\n",
       "  'room': 1,\n",
       "  'old': 3,\n",
       "  'toys': 6,\n",
       "  'one': 4,\n",
       "  'wheezy': 3,\n",
       "  'penguin': 1,\n",
       "  'broken': 2,\n",
       "  'squeaker': 1,\n",
       "  'woody': 19,\n",
       "  'tom': 2,\n",
       "  'hanks': 2,\n",
       "  'saddles': 1,\n",
       "  'dog': 2,\n",
       "  'rides': 1,\n",
       "  'rescue': 1,\n",
       "  'succeeds': 1,\n",
       "  'mission': 1,\n",
       "  'make': 1,\n",
       "  'back': 1,\n",
       "  'house': 1,\n",
       "  'al': 7,\n",
       "  'unscrupulous': 1,\n",
       "  'owner': 2,\n",
       "  'toy': 16,\n",
       "  'barn': 3,\n",
       "  'recognizes': 1,\n",
       "  'rare': 1,\n",
       "  'collector': 1,\n",
       "  'item': 1,\n",
       "  'steals': 1,\n",
       "  'buzz': 10,\n",
       "  'lightyear': 1,\n",
       "  'tim': 1,\n",
       "  'allen': 1,\n",
       "  'leads': 2,\n",
       "  'hamm': 1,\n",
       "  'john': 1,\n",
       "  'ratzenberger': 1,\n",
       "  'mr': 1,\n",
       "  'potato': 1,\n",
       "  'head': 1,\n",
       "  'rickles': 1,\n",
       "  'slinky': 1,\n",
       "  'jim': 1,\n",
       "  'varney': 1,\n",
       "  'rex': 1,\n",
       "  'wallace': 1,\n",
       "  'shawn': 1,\n",
       "  'city': 1,\n",
       "  'find': 1,\n",
       "  'friend': 1,\n",
       "  'meanwhile': 1,\n",
       "  'discovers': 1,\n",
       "  'reason': 1,\n",
       "  'kidnapped': 1,\n",
       "  'collected': 1,\n",
       "  'every': 2,\n",
       "  'piece': 1,\n",
       "  'merchandising': 2,\n",
       "  '1950': 1,\n",
       "  'tv': 1,\n",
       "  'puppet': 1,\n",
       "  'show': 1,\n",
       "  'round': 2,\n",
       "  'except': 1,\n",
       "  'doll': 1,\n",
       "  'collection': 1,\n",
       "  'complete': 1,\n",
       "  'plans': 1,\n",
       "  'sell': 1,\n",
       "  'gang': 2,\n",
       "  'bullseye': 1,\n",
       "  'horse': 1,\n",
       "  'jessie': 2,\n",
       "  'cowgirl': 1,\n",
       "  'joan': 1,\n",
       "  'cusack': 1,\n",
       "  'stinky': 2,\n",
       "  'pete': 2,\n",
       "  'prospector': 2,\n",
       "  'kelsey': 2,\n",
       "  'grammer': 3,\n",
       "  'museum': 1,\n",
       "  'japan': 2,\n",
       "  'happy': 2,\n",
       "  'move': 1,\n",
       "  'storage': 1,\n",
       "  'years': 1,\n",
       "  'waiting': 1,\n",
       "  'go': 3,\n",
       "  'closed': 1,\n",
       "  'dark': 2,\n",
       "  'box': 3,\n",
       "  'possibly': 1,\n",
       "  'forever': 1,\n",
       "  'trying': 1,\n",
       "  'convince': 1,\n",
       "  'stay': 1,\n",
       "  'tells': 1,\n",
       "  'abandoned': 2,\n",
       "  'grew': 1,\n",
       "  'realizes': 1,\n",
       "  'days': 1,\n",
       "  'beloved': 1,\n",
       "  'numbered': 1,\n",
       "  'ponders': 1,\n",
       "  'whether': 1,\n",
       "  'home': 1,\n",
       "  'boys': 1,\n",
       "  'invade': 1,\n",
       "  'know': 2,\n",
       "  'replaced': 1,\n",
       "  'another': 2,\n",
       "  'new': 2,\n",
       "  'generates': 1,\n",
       "  'movie': 5,\n",
       "  'biggest': 1,\n",
       "  'laughs': 2,\n",
       "  'assault': 2,\n",
       "  'zurg': 1,\n",
       "  'fortress': 1,\n",
       "  'e': 1,\n",
       "  'apartment': 2,\n",
       "  'building': 2,\n",
       "  'original': 4,\n",
       "  'story': 8,\n",
       "  'central': 1,\n",
       "  'themes': 2,\n",
       "  'reflected': 1,\n",
       "  'grown': 2,\n",
       "  'sensibility': 1,\n",
       "  'rather': 2,\n",
       "  'usual': 1,\n",
       "  'think': 1,\n",
       "  'kids': 2,\n",
       "  'want': 1,\n",
       "  'hear': 1,\n",
       "  'movies': 1,\n",
       "  'take': 1,\n",
       "  'position': 1,\n",
       "  'person': 1,\n",
       "  'special': 2,\n",
       "  'extraordinary': 1,\n",
       "  'destiny': 1,\n",
       "  'fulfill': 1,\n",
       "  'discovered': 1,\n",
       "  'like': 4,\n",
       "  'everyone': 1,\n",
       "  'else': 1,\n",
       "  'sank': 1,\n",
       "  'suicidal': 1,\n",
       "  'depression': 1,\n",
       "  'showed': 1,\n",
       "  'worthwhile': 1,\n",
       "  'loved': 1,\n",
       "  'makes': 2,\n",
       "  'child': 1,\n",
       "  'words': 1,\n",
       "  'okay': 1,\n",
       "  'ordinary': 1,\n",
       "  'love': 1,\n",
       "  'purpose': 1,\n",
       "  'life': 2,\n",
       "  'us': 1,\n",
       "  'probably': 1,\n",
       "  'come': 2,\n",
       "  'conclusion': 2,\n",
       "  'realize': 1,\n",
       "  'space': 1,\n",
       "  'rangers': 1,\n",
       "  'never': 1,\n",
       "  'going': 1,\n",
       "  '2': 5,\n",
       "  'addresses': 1,\n",
       "  'question': 1,\n",
       "  'raised': 1,\n",
       "  'drawn': 1,\n",
       "  'first': 6,\n",
       "  'forced': 1,\n",
       "  'recognize': 1,\n",
       "  'grow': 1,\n",
       "  'forget': 1,\n",
       "  'likely': 1,\n",
       "  'discarded': 1,\n",
       "  'sealed': 1,\n",
       "  'attic': 1,\n",
       "  'big': 1,\n",
       "  'theme': 2,\n",
       "  'everything': 1,\n",
       "  'ends': 1,\n",
       "  'introduced': 1,\n",
       "  'early': 1,\n",
       "  'arm': 1,\n",
       "  'torn': 1,\n",
       "  'places': 1,\n",
       "  'shelf': 2,\n",
       "  'taking': 1,\n",
       "  'meets': 1,\n",
       "  'dusty': 1,\n",
       "  'forgotten': 1,\n",
       "  'shocked': 1,\n",
       "  'shelved': 1,\n",
       "  'worry': 1,\n",
       "  'ride': 1,\n",
       "  '25': 1,\n",
       "  'cent': 1,\n",
       "  'suicide': 1,\n",
       "  'attempt': 1,\n",
       "  'stolen': 1,\n",
       "  'decide': 1,\n",
       "  'long': 2,\n",
       "  'display': 1,\n",
       "  'case': 1,\n",
       "  'uncertain': 1,\n",
       "  'future': 1,\n",
       "  'pals': 1,\n",
       "  'course': 1,\n",
       "  'spite': 1,\n",
       "  'heavy': 1,\n",
       "  'explores': 1,\n",
       "  'also': 3,\n",
       "  'funny': 2,\n",
       "  'brief': 1,\n",
       "  'substitution': 1,\n",
       "  'still': 1,\n",
       "  'delusional': 1,\n",
       "  'welcome': 1,\n",
       "  'surprise': 1,\n",
       "  'wondered': 1,\n",
       "  'sequel': 2,\n",
       "  'could': 1,\n",
       "  'without': 1,\n",
       "  'pompous': 1,\n",
       "  'attitude': 1,\n",
       "  'displayed': 1,\n",
       "  'vocal': 1,\n",
       "  'performances': 1,\n",
       "  'rate': 1,\n",
       "  'stands': 2,\n",
       "  'level': 2,\n",
       "  'enthusiasm': 1,\n",
       "  'puts': 1,\n",
       "  'work': 2,\n",
       "  'however': 1,\n",
       "  'voice': 3,\n",
       "  'horribly': 1,\n",
       "  'miscast': 1,\n",
       "  'sounds': 1,\n",
       "  'nothing': 1,\n",
       "  'although': 1,\n",
       "  'lot': 1,\n",
       "  'commercials': 1,\n",
       "  'simpsons': 1,\n",
       "  'sideshow': 1,\n",
       "  'bob': 1,\n",
       "  'slight': 2,\n",
       "  'variations': 1,\n",
       "  'range': 1,\n",
       "  'pull': 1,\n",
       "  'role': 1,\n",
       "  'animation': 1,\n",
       "  'even': 1,\n",
       "  'better': 1,\n",
       "  'especially': 1,\n",
       "  'impressive': 1,\n",
       "  'textures': 1,\n",
       "  'figures': 1,\n",
       "  'see': 2,\n",
       "  'difference': 1,\n",
       "  'flesh': 1,\n",
       "  'plastic': 1,\n",
       "  'people': 1,\n",
       "  'looked': 1,\n",
       "  'human': 1,\n",
       "  'skin': 1,\n",
       "  'looks': 1,\n",
       "  'soft': 1,\n",
       "  'bluish': 1,\n",
       "  'tint': 1,\n",
       "  'bo': 1,\n",
       "  'peep': 1,\n",
       "  'porcelain': 1,\n",
       "  'tell': 1,\n",
       "  'winded': 1,\n",
       "  'phrasing': 1,\n",
       "  'common': 1,\n",
       "  '--': 2,\n",
       "  'disney': 1,\n",
       "  'pixar': 2,\n",
       "  'coming': 1,\n",
       "  'party': 1,\n",
       "  'celebrate': 1,\n",
       "  'appended': 1,\n",
       "  'project': 1,\n",
       "  'hilarious': 1,\n",
       "  'short': 2,\n",
       "  'two': 1,\n",
       "  'desklamps': 1,\n",
       "  'rubber': 1,\n",
       "  'ball': 1,\n",
       "  'beginning': 1,\n",
       "  'made': 1,\n",
       "  '1986': 1,\n",
       "  'incredible': 1,\n",
       "  'considers': 1,\n",
       "  'relatively': 1,\n",
       "  'primitive': 1,\n",
       "  'computer': 1,\n",
       "  'technology': 1,\n",
       "  'available': 1,\n",
       "  'downside': 1,\n",
       "  'follows': 1,\n",
       "  'formula': 1,\n",
       "  'closely': 1,\n",
       "  'comes': 1,\n",
       "  'bit': 1,\n",
       "  'stale': 1,\n",
       "  'sections': 1,\n",
       "  'tend': 1,\n",
       "  'drag': 1,\n",
       "  'overall': 1,\n",
       "  'though': 1,\n",
       "  'great': 1,\n",
       "  'time': 1,\n",
       "  'found': 1,\n",
       "  'worthy': 1,\n",
       "  'successor': 1,\n",
       "  'bottom': 1,\n",
       "  'line': 1,\n",
       "  'really': 1,\n",
       "  'fun': 1,\n",
       "  'whole': 1,\n",
       "  'family': 1},\n",
       " 'pos')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model\n",
    "\n",
    "Now that our data is ready, the training step itself is quite simple if we use the [NaiveBayesClassifier](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.naivebayes) provided by NLTK.\n",
    "\n",
    "If you are used to methods such as `model.fit(x,y)` which take two parameters -- the data and the labels -- it may be confusing that `NaiveBayesClassifier.train` takes just one argument. This is because the labels are already embedded in `train_set`: each element in the set is a Bag of Words paired with a 'pos' or 'neg'; value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "\n",
    "model = NaiveBayesClassifier.train(train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check model accuracy\n",
    "\n",
    "NLTK's built-in [accuracy](https://www.nltk.org/api/nltk.classify.html#module-nltk.classify.util) utility can run our test_set through the model and compare the labels returned by the model to the labels in the test set, producing an overall % accuracy. Not too impressive, right? We need to improve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67.5\n"
     ]
    }
   ],
   "source": [
    "from nltk.classify.util import accuracy\n",
    "\n",
    "print(100 * accuracy(model, test_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model\n",
    "Our trained model will be cleared from memory when this notebook is closed. So that we can use it again later, save the model as a file using the [pickle](https://docs.python.org/3/library/pickle.html) serializer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "model_file = open('sa_classifier.pickle','wb')\n",
    "pickle.dump(model, model_file)\n",
    "model_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying a different model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from nltk.stem import LancasterStemmer,WordNetLemmatizer\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report,confusion_matrix,accuracy_score\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turn data set into dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as mr\n",
    "\n",
    "reviews = []\n",
    "for fileid in mr.fileids():\n",
    "    tag, filename = fileid.split('/')\n",
    "    reviews.append((mr.raw(fileid), tag))\n",
    "\n",
    "df = pd.DataFrame(reviews, columns=['review', 'sentiment'])\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>2000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>for those of us who weren't yet born when the ...</td>\n",
       "      <td>pos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>1</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   review sentiment\n",
       "count                                                2000      2000\n",
       "unique                                               2000         2\n",
       "top     for those of us who weren't yet born when the ...       pos\n",
       "freq                                                    1      1000"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pos    1000\n",
       "neg    1000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above create a randomized dataframe of all the movie reviews with two columns, one of the review, the other of the text. Next step is to clean the reviews so that they can be turned into a better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=ToktokTokenizer()\n",
    "stopword_list=nltk.corpus.stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "import re\n",
    "\n",
    "def remove_special_characters(text, remove_digits=True):\n",
    "    pattern=r'[^a-zA-z0-9\\s]'\n",
    "    text=re.sub(pattern,'',text)\n",
    "    return text\n",
    "\n",
    "def simple_lemmer(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    word_list = nltk.word_tokenize(text)\n",
    "    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n",
    "    return lemmatized_output\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens) \n",
    "    return filtered_text\n",
    "\n",
    "df['review'] = df['review'].apply(remove_special_characters)\n",
    "df['review'] = df['review'].apply(simple_lemmer)\n",
    "df['review'] = df['review'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'following review contains harsh language expect clicked title cast kristen holly smith danica sheridan alex boling michael dotson sonya hensley janet krajeski sabrina lu dionysius burbano calvin grant jeff b harmon written directed jeff b harmon running time 97 minute thought losing make vomity inside blatz balinski danica sheridan lament fact lesbian lover april kristen holly smith ha received telegram exfiance isle lesbos incredibly offensive musical comedy april pfferpot smith resident small town bumfuck arkansas get married high school sweetheart football hero dick dickson michael dotson april get extreme cold foot run home stick gun mouth pull trigger instead killing magically transported mirror isle lesbos alternate dimension lesbian rule men allowed except lance homosexual toilet cleanerslave april love new home friend dick parent ready give mr pfferpot director jeff b harmon janet krajeski decide need medical help enlist aid dr sigmoid colon also jeff b harmon claim cure homosexuality actuality dr colon homosexual well begin special treatment dick dickson unbeknownst dick april turn dick demand return bumfuck decides take matter hand attack isle lesbos rambostyle instead leveling place fall love lance alex boling two along april lover blatz get married note filmmaker character lance make aside camera followed silence im assuming wa inserted purpose waiting laughter audience subsided april parent feeling like recourse call favor president clinton send nuclear bomb whose circuitry inexplicably made homosexual performer way bomb dud thanks work circuitry rewire send back washington c destroying mr pfferpot give trying get daughter back instead decide join alternative sexual practice im sure writerdirectorcostar jeff b harmon wa purposely trying offend people belief idea movie present wa going cheap laugh either way manages present offensive material ever seen movie film open preacher running small africanamerican child town later move hanging michael jackson impersonator ku klux klan jaunty musical number preceded remark gay straight finally able put difference behind work together hate others jew harmon wa merely trying point idiotic society apologize harsh element film presented mean spiritedly cant help feel like serious intent entire central theme movie im afraid one dont get tried keep open mind watching homosexual element prevalent taste wa hard enough watch bash race sex without preaching virtue homosexual accepted society doe one expect accepted guilty nonacceptance im prone judging anyone believe people free explore whatever avenue wish free scornful eye society dont force particular rhetoric throat arent going show respect preference wish maybe im missing satirical point harmon trying make think could handled little tactfully far musical comedy go isle lesbos trey parker matt stone musical good portion song maddeningly catchy despite disturbing visuals mom apple pie stuck rest day speaking disturbing visuals wedding bell aint ringing could decent song accompanying visuals spousal abuse harrowing film displaying im lesbian touted press release particularly popular also pretty good good portion could due stellar singing voice smith mainly excellent vocal rating film based performer decent save rosie odonnell like performance given danica sheridan inherent problem character singing voice leaf little desired one particular musical number lesbian rock wa one lowest point ever experienced filmgoing life thanks strained lackluster vocal job sheridan film end twenty minute credit actually roll remainder running time padded song lesbian rock included ridiculously presented antinukes message tacked end reason make feature length film isle lesbos reprise wa reached felt story already wrapped well enough end wa film needlessly stretched past obvious end isle lesbos available videocassette www indieunderground com transfer pretty clean detail wrinkled cloth paint backdrop readily apparent film letterboxed approximately 1 85 1 many respect isle lesbos ha incredible cult potential like rocky horror new millennium film mean cup tea know enjoy think might mean seek hand cleansing palette good action film like gladiator wait arent gladiator film considered oh never mind'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[69]['review']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the test/train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600,) (1600,)\n",
      "(400,) (400,)\n"
     ]
    }
   ],
   "source": [
    "# Create Training Set\n",
    "train_reviews=df.review[:1600]\n",
    "train_sentiments=df.sentiment[:1600]\n",
    "\n",
    "# Create Test Set\n",
    "test_reviews=df.review[1600:]\n",
    "test_sentiments=df.sentiment[1600:]\n",
    "\n",
    "# Make sure things are the same\n",
    "print(train_reviews.shape,train_sentiments.shape)\n",
    "print(test_reviews.shape,test_sentiments.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to turn those reviews into vectors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df = 5,\n",
    "                             max_df = 0.8,\n",
    "                             sublinear_tf = True,\n",
    "                             use_idf = True)\n",
    "\n",
    "train_vectors = vectorizer.fit_transform(train_reviews)\n",
    "test_vectors = vectorizer.transform(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 4.902939s; Prediction time: 1.011080s\n",
      "positive:  {'precision': 0.9024390243902439, 'recall': 0.9203980099502488, 'f1-score': 0.9113300492610837, 'support': 201}\n",
      "negative:  {'precision': 0.9179487179487179, 'recall': 0.8994974874371859, 'f1-score': 0.9086294416243655, 'support': 199}\n"
     ]
    }
   ],
   "source": [
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "t0 = time.time()\n",
    "classifier_linear.fit(train_vectors, train_sentiments)\n",
    "t1 = time.time()\n",
    "prediction_linear = classifier_linear.predict(test_vectors)\n",
    "t2 = time.time()\n",
    "time_linear_train = t1-t0\n",
    "time_linear_predict = t2-t1\n",
    "\n",
    "print(\"Training time: %fs; Prediction time: %fs\" % (time_linear_train, time_linear_predict))\n",
    "\n",
    "report = classification_report(test_sentiments, prediction_linear, output_dict=True)\n",
    "print('positive: ', report['pos'])\n",
    "\n",
    "print('negative: ', report['neg'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the model (Colab version)\n",
    "\n",
    "Google Colab doesn't provide direct access to files saved during a notebook session, so we need to save it in [Google Drive](https://drive.google.com) instead. The first time you run this, it will ask for permission to access your Google Drive. Follow the instructions, then wait a few minutes and look for a new folder called \"Colab Output\" in [Drive](https://drive.google.com). Note that Colab does not alway sync to Drive immediately, so check the file update times and re-run this cell if it doesn't look like you have the most revent version of your file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/gdrive')\n",
    "    !mkdir -p '/content/gdrive/My Drive/Colab Output'\n",
    "    model_file = open('/content/gdrive/My Drive/Colab Output/sa_classifier.pickle','wb')\n",
    "    pickle.dump(model, model_file)\n",
    "    model_file.flush()\n",
    "    print('Model saved in /content/gdrive/My Drive/Colab Output')\n",
    "    !ls '/content/gdrive/My Drive/Colab Output'\n",
    "    drive.flush_and_unmount()\n",
    "    print('Re-run this cell if you cannot find it in https://drive.google.com')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
